{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path,filename)\n",
    "with http.request('GET', url, preload_content=False) as r,open(zipfilename,'wb') as out_file:\n",
    "    shutil.copyfileobj(r,out_file)\n",
    "with zipfile.ZipFile(zipfilename,'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 194513\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt',names=['src','tar','lic'],sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수 :',len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39580</th>\n",
       "      <td>Why are you with me?</td>\n",
       "      <td>Pourquoi êtes-vous avec moi ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25500</th>\n",
       "      <td>She's still young.</td>\n",
       "      <td>Elle est encore jeune.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14521</th>\n",
       "      <td>I'll allow this.</td>\n",
       "      <td>Je le permettrai.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8831</th>\n",
       "      <td>Tom liked you.</td>\n",
       "      <td>Tom t'aimait bien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36031</th>\n",
       "      <td>I stole it from Tom.</td>\n",
       "      <td>Je l'ai volé à Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9201</th>\n",
       "      <td>We're touched.</td>\n",
       "      <td>Nous sommes touchés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41933</th>\n",
       "      <td>I eat here every day.</td>\n",
       "      <td>Je mange ici tous les jours.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37956</th>\n",
       "      <td>The lights went out.</td>\n",
       "      <td>Les lumières se sont éteintes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36439</th>\n",
       "      <td>I'm fine, thank you.</td>\n",
       "      <td>Ça va, merci.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44450</th>\n",
       "      <td>That doesn't help me.</td>\n",
       "      <td>Ça ne m'aide pas.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         src                             tar\n",
       "39580   Why are you with me?   Pourquoi êtes-vous avec moi ?\n",
       "25500     She's still young.          Elle est encore jeune.\n",
       "14521       I'll allow this.               Je le permettrai.\n",
       "8831          Tom liked you.              Tom t'aimait bien.\n",
       "36031   I stole it from Tom.             Je l'ai volé à Tom.\n",
       "9201          We're touched.            Nous sommes touchés.\n",
       "41933  I eat here every day.    Je mange ici tous les jours.\n",
       "37956   The lights went out.  Les lumières se sont éteintes.\n",
       "36439   I'm fine, thank you.                   Ça va, merci.\n",
       "44450  That doesn't help me.               Ça ne m'aide pas."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:,'src':'tar']\n",
    "lines = lines[0:60000]\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11746</th>\n",
       "      <td>That's the one.</td>\n",
       "      <td>\\tC'est celui-là.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27740</th>\n",
       "      <td>You're disgusting.</td>\n",
       "      <td>\\tVous êtes dégoûtant.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31808</th>\n",
       "      <td>They came together.</td>\n",
       "      <td>\\tElles sont venues ensemble.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17135</th>\n",
       "      <td>You're the best.</td>\n",
       "      <td>\\tVous êtes le meilleur.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18011</th>\n",
       "      <td>He shares a room.</td>\n",
       "      <td>\\tIl partage une chambre.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36328</th>\n",
       "      <td>I'll do my homework.</td>\n",
       "      <td>\\tJe ferai mes devoirs.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48742</th>\n",
       "      <td>I could never do that.</td>\n",
       "      <td>\\tJe ne pourrais jamais faire cela.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>That's stupid.</td>\n",
       "      <td>\\tC'est bête.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>Please phone him.</td>\n",
       "      <td>\\tVeuillez l'appeler.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45118</th>\n",
       "      <td>This place is a dump.</td>\n",
       "      <td>\\tCet endroit est une décharge.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                    tar\n",
       "11746         That's the one.                    \\tC'est celui-là.\\n\n",
       "27740      You're disgusting.               \\tVous êtes dégoûtant.\\n\n",
       "31808     They came together.        \\tElles sont venues ensemble.\\n\n",
       "17135        You're the best.             \\tVous êtes le meilleur.\\n\n",
       "18011       He shares a room.            \\tIl partage une chambre.\\n\n",
       "36328    I'll do my homework.              \\tJe ferai mes devoirs.\\n\n",
       "48742  I could never do that.  \\tJe ne pourrais jamais faire cela.\\n\n",
       "8533           That's stupid.                        \\tC'est bête.\\n\n",
       "19990       Please phone him.                \\tVeuillez l'appeler.\\n\n",
       "45118   This place is a dump.      \\tCet endroit est une décharge.\\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x: '\\t' + x + '\\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 80\n",
      "target 문장의 char 집합 : 105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :',src_vocab_size)\n",
    "print('target 문장의 char 집합 :',tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '°': 76, 'é': 77, '’': 78, '€': 79}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word,i+1) for i,word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word,i+1) for i,word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    encoded_line = []\n",
    "    \n",
    "    for char in line:\n",
    "        encoded_line.append(src_to_index[char])\n",
    "    encoder_input.append(encoded_line)\n",
    "print('source 문장의 정수 인코딩 :',encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[1, 48, 53, 3, 4, 2], [1, 39, 53, 70, 55, 60, 57, 14, 2], [1, 28, 67, 73, 59, 57, 3, 4, 2], [1, 45, 53, 64, 73, 72, 3, 4, 2], [1, 45, 53, 64, 73, 72, 14, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        encoded_line.append(tar_to_index[char])\n",
    "    decoder_input.append(encoded_line)\n",
    "print('target 문장의 정수 인코딩 :',decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[48, 53, 3, 4, 2], [39, 53, 70, 55, 60, 57, 14, 2], [28, 67, 73, 59, 57, 3, 4, 2], [45, 53, 64, 73, 72, 3, 4, 2], [45, 53, 64, 73, 72, 14, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    timestep = 0\n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        if timestep > 0:\n",
    "            encoded_line.append(tar_to_index[char])\n",
    "        timestep = timestep + 1\n",
    "    decoder_target.append(encoded_line)\n",
    "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 23\n",
      "target 문장의 최대 길이 : 74\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('source 문장의 최대 길이 :',max_src_len)\n",
    "print('target 문장의 최대 길이 :',max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input,maxlen=max_src_len,padding='post')\n",
    "decoder_input = pad_sequences(decoder_input,maxlen=max_tar_len,padding='post')\n",
    "decoder_target = pad_sequences(decoder_target,maxlen=max_tar_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256,return_state=True)\n",
    "\n",
    "encoder_outputs,state_h,state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256,return_sequences=True,return_state=True)\n",
    "\n",
    "decoder_outputs,_,_ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size,activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compile(optimizer='rmsprop',loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "750/750 [==============================] - 23s 17ms/step - loss: 0.7582 - val_loss: 0.6772\n",
      "Epoch 2/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.4687 - val_loss: 0.5520\n",
      "Epoch 3/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.3941 - val_loss: 0.4885\n",
      "Epoch 4/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.3511 - val_loss: 0.4526\n",
      "Epoch 5/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.3220 - val_loss: 0.4263\n",
      "Epoch 6/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.3006 - val_loss: 0.4088\n",
      "Epoch 7/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2843 - val_loss: 0.3967\n",
      "Epoch 8/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2705 - val_loss: 0.3872\n",
      "Epoch 9/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2591 - val_loss: 0.3787\n",
      "Epoch 10/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2494 - val_loss: 0.3730\n",
      "Epoch 11/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2409 - val_loss: 0.3724\n",
      "Epoch 12/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2333 - val_loss: 0.3694\n",
      "Epoch 13/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2265 - val_loss: 0.3659\n",
      "Epoch 14/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2204 - val_loss: 0.3638\n",
      "Epoch 15/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2146 - val_loss: 0.3621\n",
      "Epoch 16/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2094 - val_loss: 0.3641\n",
      "Epoch 17/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2045 - val_loss: 0.3640\n",
      "Epoch 18/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1999 - val_loss: 0.3646\n",
      "Epoch 19/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1957 - val_loss: 0.3651\n",
      "Epoch 20/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1916 - val_loss: 0.3667\n",
      "Epoch 21/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1876 - val_loss: 0.3674\n",
      "Epoch 22/40\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1840 - val_loss: 0.3687\n",
      "Epoch 23/40\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1806 - val_loss: 0.3702\n",
      "Epoch 24/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1773 - val_loss: 0.3723\n",
      "Epoch 25/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1743 - val_loss: 0.3746\n",
      "Epoch 26/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1711 - val_loss: 0.3775\n",
      "Epoch 27/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1683 - val_loss: 0.3799\n",
      "Epoch 28/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1656 - val_loss: 0.3834\n",
      "Epoch 29/40\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1629 - val_loss: 0.3864\n",
      "Epoch 30/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1606 - val_loss: 0.3868\n",
      "Epoch 31/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1580 - val_loss: 0.3888\n",
      "Epoch 32/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1557 - val_loss: 0.3906\n",
      "Epoch 33/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1535 - val_loss: 0.3952\n",
      "Epoch 34/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1514 - val_loss: 0.3994\n",
      "Epoch 35/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1494 - val_loss: 0.4013\n",
      "Epoch 36/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1474 - val_loss: 0.4028\n",
      "Epoch 37/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1454 - val_loss: 0.4071\n",
      "Epoch 38/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1437 - val_loss: 0.4103\n",
      "Epoch 39/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1418 - val_loss: 0.4117\n",
      "Epoch 40/40\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1403 - val_loss: 0.4166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff86d37d60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input,decoder_input],y=decoder_target,batch_size=64,epochs=40,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs,outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs,state_h,state_c = decoder_lstm(decoder_inputs,initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i,char) for char,i in src_to_index.items())\n",
    "index_to_tar = dict((i,char) for char,i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1,tar_vocab_size))\n",
    "    target_seq[0,0,tar_to_index['\\t']] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1,1,tar_vocab_size))\n",
    "        target_seq[0,0,sampled_token_index] = 1.\n",
    "        \n",
    "        states_value=[h,c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Hi.\n",
      "정답 문장: Salut !\n",
      "번역 문장: Salut !\n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장: Salut !\n",
      "번역 문장: Bonjour !\n",
      "-----------------------------------\n",
      "입력 문장: Hop in.\n",
      "정답 문장: Montez.\n",
      "번역 문장: Monte les filles !\n",
      "-----------------------------------\n",
      "입력 문장: Help me!\n",
      "정답 문장: Aide-moi !\n",
      "번역 문장: Aidez-moi !\n",
      "-----------------------------------\n",
      "입력 문장: How's Tom?\n",
      "정답 문장: Comment va Tom ?\n",
      "번역 문장: Comment Tom va-t-il ?\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1000]:\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * '-')\n",
    "    print(\"입력 문장:\",lines.src[seq_index])\n",
    "    print(\"정답 문장:\",lines.tar[seq_index][1:len(lines.tar[seq_index])-1])\n",
    "    print(\"번역 문장:\",decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
    "    shutil.copyfileobj(r, out_file)\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii(s):\n",
    "    \n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "def preprocess_sentence(sent):\n",
    "\n",
    "    sent = to_ascii(sent.lower())\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print(\"전처리 전 영어 문장 :\",en_sent)\n",
    "print(\"전처리 후 영어 문장 :\",preprocess_sentence(en_sent))\n",
    "print(\"전처리 전 프랑스어 문장 :\",fr_sent)\n",
    "print(\"전처리 후 프랑스어 문장 :\",preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"fra.txt\", \"r\", encoding='UTF8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\",lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input,padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\",lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input,padding='post')\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 8)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4672, 프랑스어 단어 집합의 크기 : 8153\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [12613 21471 31803 ... 23026 28642  3315]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43,  56, 237,   1,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2, 814,  40, 349,   9,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([814,  40, 349,   9,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 8)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 8)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim =64\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size,embedding_dim)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(hidden_units,return_state=True)\n",
    "encoder_outputs,state_h,state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size,hidden_units)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "decoder_lstm = LSTM(hidden_units,return_sequences=True,return_state=True)\n",
    "\n",
    "decoder_outputs,_,_ = decoder_lstm(dec_masking,initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(tar_vocab_size,activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/233 [==============================] - 21s 43ms/step - loss: 3.3368 - acc: 0.6140 - val_loss: 2.0288 - val_acc: 0.6174\n",
      "Epoch 2/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 1.8641 - acc: 0.6633 - val_loss: 1.7589 - val_acc: 0.7080\n",
      "Epoch 3/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.6606 - acc: 0.7388 - val_loss: 1.5922 - val_acc: 0.7580\n",
      "Epoch 4/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.5080 - acc: 0.7601 - val_loss: 1.4763 - val_acc: 0.7630\n",
      "Epoch 5/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.4103 - acc: 0.7667 - val_loss: 1.4063 - val_acc: 0.7727\n",
      "Epoch 6/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.3388 - acc: 0.7778 - val_loss: 1.3489 - val_acc: 0.7816\n",
      "Epoch 7/50\n",
      "233/233 [==============================] - 7s 31ms/step - loss: 1.2780 - acc: 0.7864 - val_loss: 1.3003 - val_acc: 0.7912\n",
      "Epoch 8/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 1.2194 - acc: 0.7983 - val_loss: 1.2551 - val_acc: 0.7997\n",
      "Epoch 9/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.1669 - acc: 0.8058 - val_loss: 1.2131 - val_acc: 0.8066\n",
      "Epoch 10/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 1.1188 - acc: 0.8134 - val_loss: 1.1769 - val_acc: 0.8125\n",
      "Epoch 11/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 1.0759 - acc: 0.8186 - val_loss: 1.1448 - val_acc: 0.8159\n",
      "Epoch 12/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.0366 - acc: 0.8233 - val_loss: 1.1157 - val_acc: 0.8194\n",
      "Epoch 13/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.9999 - acc: 0.8281 - val_loss: 1.0916 - val_acc: 0.8234\n",
      "Epoch 14/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.9661 - acc: 0.8322 - val_loss: 1.0699 - val_acc: 0.8251\n",
      "Epoch 15/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.9346 - acc: 0.8357 - val_loss: 1.0419 - val_acc: 0.8289\n",
      "Epoch 16/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.8999 - acc: 0.8400 - val_loss: 1.0208 - val_acc: 0.8313\n",
      "Epoch 17/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.8686 - acc: 0.8433 - val_loss: 1.0028 - val_acc: 0.8329\n",
      "Epoch 18/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.8386 - acc: 0.8466 - val_loss: 0.9842 - val_acc: 0.8363\n",
      "Epoch 19/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.8108 - acc: 0.8498 - val_loss: 0.9652 - val_acc: 0.8380\n",
      "Epoch 20/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.7843 - acc: 0.8527 - val_loss: 0.9518 - val_acc: 0.8402\n",
      "Epoch 21/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.7588 - acc: 0.8555 - val_loss: 0.9371 - val_acc: 0.8416\n",
      "Epoch 22/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.7344 - acc: 0.8583 - val_loss: 0.9249 - val_acc: 0.8438\n",
      "Epoch 23/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.7112 - acc: 0.8608 - val_loss: 0.9114 - val_acc: 0.8449\n",
      "Epoch 24/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.6887 - acc: 0.8635 - val_loss: 0.9020 - val_acc: 0.8464\n",
      "Epoch 25/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.6678 - acc: 0.8658 - val_loss: 0.8867 - val_acc: 0.8482\n",
      "Epoch 26/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.6467 - acc: 0.8682 - val_loss: 0.8764 - val_acc: 0.8494\n",
      "Epoch 27/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.6273 - acc: 0.8707 - val_loss: 0.8696 - val_acc: 0.8499\n",
      "Epoch 28/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.6081 - acc: 0.8732 - val_loss: 0.8600 - val_acc: 0.8509\n",
      "Epoch 29/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.5903 - acc: 0.8754 - val_loss: 0.8545 - val_acc: 0.8523\n",
      "Epoch 30/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.5724 - acc: 0.8782 - val_loss: 0.8462 - val_acc: 0.8531\n",
      "Epoch 31/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.5564 - acc: 0.8802 - val_loss: 0.8375 - val_acc: 0.8542\n",
      "Epoch 32/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.5404 - acc: 0.8827 - val_loss: 0.8318 - val_acc: 0.8555\n",
      "Epoch 33/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.5247 - acc: 0.8853 - val_loss: 0.8280 - val_acc: 0.8557\n",
      "Epoch 34/50\n",
      "233/233 [==============================] - 7s 31ms/step - loss: 0.5093 - acc: 0.8876 - val_loss: 0.8205 - val_acc: 0.8568\n",
      "Epoch 35/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4955 - acc: 0.8899 - val_loss: 0.8169 - val_acc: 0.8574\n",
      "Epoch 36/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4816 - acc: 0.8922 - val_loss: 0.8113 - val_acc: 0.8586\n",
      "Epoch 37/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.4687 - acc: 0.8943 - val_loss: 0.8079 - val_acc: 0.8597\n",
      "Epoch 38/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.4566 - acc: 0.8964 - val_loss: 0.8065 - val_acc: 0.8598\n",
      "Epoch 39/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.4439 - acc: 0.8985 - val_loss: 0.8033 - val_acc: 0.8602\n",
      "Epoch 40/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.4330 - acc: 0.9004 - val_loss: 0.8014 - val_acc: 0.8602\n",
      "Epoch 41/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4229 - acc: 0.9021 - val_loss: 0.8008 - val_acc: 0.8611\n",
      "Epoch 42/50\n",
      "233/233 [==============================] - 7s 31ms/step - loss: 0.4112 - acc: 0.9043 - val_loss: 0.7985 - val_acc: 0.8619\n",
      "Epoch 43/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4002 - acc: 0.9065 - val_loss: 0.7939 - val_acc: 0.8630\n",
      "Epoch 44/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.3899 - acc: 0.9083 - val_loss: 0.7952 - val_acc: 0.8629\n",
      "Epoch 45/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3813 - acc: 0.9099 - val_loss: 0.7929 - val_acc: 0.8642\n",
      "Epoch 46/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3724 - acc: 0.9113 - val_loss: 0.7905 - val_acc: 0.8659\n",
      "Epoch 47/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3622 - acc: 0.9134 - val_loss: 0.7907 - val_acc: 0.8650\n",
      "Epoch 48/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3532 - acc: 0.9154 - val_loss: 0.7916 - val_acc: 0.8650\n",
      "Epoch 49/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.3456 - acc: 0.9171 - val_loss: 0.7921 - val_acc: 0.8646\n",
      "Epoch 50/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.3367 - acc: 0.9185 - val_loss: 0.7878 - val_acc: 0.8661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eba4b719a0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train,decoder_input_train],y=decoder_target_train, \\\n",
    "         validation_data=([encoder_input_test,decoder_input_test],decoder_target_test),\n",
    "         batch_size=128,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_outputs2,state_h2,state_c2 = decoder_lstm(dec_emb2,initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2,state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = tar_to_index['<sos>']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence = \" \"+sampled_char\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
    "            stop_condition =True\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "        states_value = [h,c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 정수 생성\n",
    "  target_seq = np.zeros((1,1))\n",
    "  target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "  while not stop_condition:\n",
    "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # 예측 결과를 단어로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "    decoded_sentence += ' '+sampled_char\n",
    "\n",
    "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "    if (sampled_char == '<eos>' or\n",
    "        len(decoded_sentence) > 50):\n",
    "        stop_condition = True\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "    return sentence\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0 and encoded_word !=tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "            sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : i am very pleased . \n",
      "정답문장 : je suis tres content . \n",
      "번역문장 : je suis tres heureux . \n",
      "--------------------------------------------------\n",
      "입력문장 : i have immunity . \n",
      "정답문장 : je dispose de l immunite . \n",
      "번역문장 : je dispose de l acquisition . \n",
      "--------------------------------------------------\n",
      "입력문장 : she knew the teen . \n",
      "정답문장 : elle connaissait l adolescente . \n",
      "번역문장 : elle connaissait l adolescent . \n",
      "--------------------------------------------------\n",
      "입력문장 : you re great . \n",
      "정답문장 : t assures . \n",
      "번역문장 : t es incroyable . \n",
      "--------------------------------------------------\n",
      "입력문장 : tom walked in . \n",
      "정답문장 : tom entra . \n",
      "번역문장 : tom est en train de parler . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : rest here . \n",
      "정답문장 : reposez vous ici . \n",
      "번역문장 : reviens a l argent . \n",
      "--------------------------------------------------\n",
      "입력문장 : let it go . \n",
      "정답문장 : laisse tomber ! \n",
      "번역문장 : laissez tomber ! \n",
      "--------------------------------------------------\n",
      "입력문장 : are you ready now ? \n",
      "정답문장 : etes vous prete maintenant ? \n",
      "번역문장 : etes vous maintenant prets ? \n",
      "--------------------------------------------------\n",
      "입력문장 : she made him happy . \n",
      "정답문장 : elle le rendit heureux . \n",
      "번역문장 : elle l a fait mal . \n",
      "--------------------------------------------------\n",
      "입력문장 : i m saying no . \n",
      "정답문장 : je dis non . \n",
      "번역문장 : je moi ! \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_count(tokens,n):\n",
    "    return Counter(ngrams(tokens,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니그램 카운트 : Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\n"
     ]
    }
   ],
   "source": [
    "candidate = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "tokens = candidate.split()\n",
    "result = simple_count(tokens,1)\n",
    "print('유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니그램 카운트 : Counter({('the',): 7})\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "tokens = candidate.split()\n",
    "result = simple_count(tokens,1)\n",
    "print('유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clip(candidate,reference_list,n):\n",
    "    ca_cnt = simple_count(candidate,n)\n",
    "    max_ref_cnt_dict = dict()\n",
    "    \n",
    "    for ref in reference_list:\n",
    "        ref_cnt = simple_count(ref,n)\n",
    "        \n",
    "        for n_gram in ref_cnt:\n",
    "            if n_gram in max_ref_cnt_dict:\n",
    "                max_ref_cnt_dict[n_gram] = max(ref_cnt[n_gram],max_ref_cnt_dict[n_gram])\n",
    "            else:\n",
    "                max_ref_cnt_dict[n_gram] = ref_cnt[n_gram]\n",
    "    return {\n",
    "        n_gram: min(ca_cnt.get(n_gram,0),max_ref_cnt_dict.get(n_gram,0)) for n_gram in ca_cnt\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보정된 유니그램 카운트 : {('the',): 2}\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "references = [\n",
    "    'the cat is on the mat',\n",
    "    'there is a cat on the mat'\n",
    "]\n",
    "result = count_clip(candidate.split(),list(map(lambda ref: ref.split(),references)),1)\n",
    "print('보정된 유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate,reference_list,n):\n",
    "    clip_cnt = count_clip(candidate,reference_list,n)\n",
    "    total_clip_cnt = sum(clip_cnt.values())\n",
    "    cnt = simple_count(candidate,n)\n",
    "    total_cnt = sum(cnt.values())\n",
    "    if total_cnt == 0:\n",
    "        total_cnt = 1\n",
    "    return(total_clip_cnt/total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보정된 유니그램 정밀도 : 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "result = modified_precision(candidate.split(),list(map(lambda ref:ref.split(),references)),1)\n",
    "print('보정된 유니그램 정밀도 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_ref_length(candidate,reference_list):\n",
    "    ca_len = len(candidate)\n",
    "    ref_lens = (len(ref) for ref in reference_list)\n",
    "    closest_ref_len = min(ref_lens,key=lambda ref_len: (abs(ref_len - ca_len),ref_len))\n",
    "    return closest_ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate,reference_list):\n",
    "    ca_len = len(candidate)\n",
    "    ref_len = closest_ref_length(candidate,reference_list)\n",
    "    if ca_len > ref_len:\n",
    "        return 1\n",
    "    elif ca_len == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.exp(1 - ref_len/ca_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate,reference_list,weights=[0.25,0.25,0.25,0.25]):\n",
    "    bp = brevity_penalty(candidate,reference_list)\n",
    "    p_n = [modified_precision(candidate,reference_list,n=n) for n,_ in enumerate(weights,start=1)]\n",
    "    score = np.sum([w_i*np.log(p_i) if p_i != 0 else 0 for w_i,p_i in zip(weights,p_n)])\n",
    "    return bp *np.exp(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실습 코드의 BLEU : 0.5045666840058485\n",
      "패키지 NLTK의 BLEU : 0.5045666840058485\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "references = [\n",
    "    'It is a guide to action that ensures that the military will forever heed Party commands',\n",
    "    'It is the guiding principle which guarantees the military forces always being under the command of the Party',\n",
    "    'It is the practical guide for the army always to heed the directions of the party'\n",
    "]\n",
    "print('실습 코드의 BLEU :',bleu_score(candidate.split(),list(map(lambda ref: ref.split(),references))))\n",
    "print('패키지 NLTK의 BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(),references)),candidate.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words= vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 2494\n",
      "리뷰의 평균 길이 : 238.71364\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 : {}'.format(max(len(l) for l in X_train)))\n",
    "print('리뷰의 평균 길이 : {}'.format(sum(map(len,X_train))/len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "X_train = pad_sequences(X_train,maxlen=max_len)\n",
    "X_test = pad_sequences(X_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = Dense(units)\n",
    "    self.W2 = Dense(units)\n",
    "    self.V = Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # query shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Embedding,Bidirectional,LSTM,Concatenate,Dropout\n",
    "from tensorflow.keras import Input,Model\n",
    "from tensorflow.keras import optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(max_len,),dtype='int32')\n",
    "embedded_sequences = Embedding(vocab_size,128,input_length=max_len,mask_zero=True)(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Bidirectional(LSTM(64,dropout=0.5,return_sequences=True))(embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm, forward_h,forward_c,backward_h,backward_c = Bidirectional \\\n",
    "  (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\n"
     ]
    }
   ],
   "source": [
    "print(lstm.shape,forward_h.shape,forward_c.shape,backward_h.shape,backward_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_h = Concatenate()([forward_h,backward_h])\n",
    "state_c = Concatenate()([forward_c,backward_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = BahdanauAttention(64)\n",
    "context_vector, attention_weights = attention(lstm,state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Dense(20,activation='relu')(context_vector)\n",
    "dropout = Dropout(0.5)(dense1)\n",
    "output = Dense(1,activation='sigmoid')(dropout)\n",
    "model = Model(inputs=sequence_input,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 128)     1280000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 500, 128)     98816       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  [(None, 500, 128),  98816       ['bidirectional[0][0]']          \n",
      " )                               (None, 64),                                                      \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128)          0           ['bidirectional_1[0][1]',        \n",
      "                                                                  'bidirectional_1[0][3]']        \n",
      "                                                                                                  \n",
      " bahdanau_attention (BahdanauAt  ((None, 128),       16577       ['bidirectional_1[0][0]',        \n",
      " tention)                        (None, 500, 1))                  'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 20)           2580        ['bahdanau_attention[0][0]']     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            21          ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,496,810\n",
      "Trainable params: 1,496,810\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "98/98 [==============================] - 1025s 10s/step - loss: 0.4654 - accuracy: 0.7796 - val_loss: 0.3158 - val_accuracy: 0.8670\n",
      "Epoch 2/3\n",
      "98/98 [==============================] - 1011s 10s/step - loss: 0.2446 - accuracy: 0.9148 - val_loss: 0.3009 - val_accuracy: 0.8749\n",
      "Epoch 3/3\n",
      "98/98 [==============================] - 1075s 11s/step - loss: 0.1882 - accuracy: 0.9365 - val_loss: 0.2966 - val_accuracy: 0.8802\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 759s 970ms/step - loss: 0.2966 - accuracy: 0.8802\n",
      "\n",
      " 테스트 정확도: 0.8802\n"
     ]
    }
   ],
   "source": [
    "print('\\n 테스트 정확도: %.4f'%(model.evaluate(X_test,y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
